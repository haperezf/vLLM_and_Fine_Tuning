# ğŸ“¡ vLLM + Fineâ€‘Tuning de Mistralâ€‘7B para Telecomunicaciones

Este repositorio contiene el flujo completo para descargar, afinar, fusionar y desplegar **Mistralâ€‘7Bâ€‘Instructâ€‘v0.3** con conocimientos especÃ­ficos del dominio de telecomunicaciones.

## âœ¨ Funcionalidades

- **Descarga** automÃ¡tica del modelo base desde Hugging Face.
- **Fineâ€‘tuning** con LoRA (4â€‘bit) empleando conjuntos `qa_dataset.csv` y `glosario_dataset.csv`.
- **FusiÃ³n (merge)** de los adaptadores LoRA con el modelo original.
- **Despliegue** en `vLLM` para inferencia concurrente.
- **ConversiÃ³n** a formato **GGUF** compatible con **LMÂ Studio** o `llama.cpp`.

---

## ğŸ–¥ï¸ Requisitos

| Recurso | RecomendaciÃ³n mÃ­nima |
|---------|----------------------|
| GPU     | NVIDIA RTXÂ 3080 (â‰¥16â€¯GB VRAM) |
| CPU     | 8Â nÃºcleos |
| RAM     | 32â€¯GB |
| SO      | Linux, WSLÂ 2 o WindowsÂ 10+ |
| Software| PythonÂ 3.10+, Docker |

Instala las dependencias de Python:

```bash
pip install -r requirements.txt
```

---

## ğŸ“ Estructura del proyecto

```text
.
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ qa_dataset.csv
â”‚   â””â”€â”€ glosario_dataset.csv
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ mistral/              # Modelo descargado de HF
â”‚   â”œâ”€â”€ mistral-7b-finetuned/ # Modelo despuÃ©s del fineâ€‘tuning
â”‚   â”œâ”€â”€ mistral-7b-merged/    # Modelo con LoRA fusionado
â”‚   â””â”€â”€ mistral-7b-gguf/      # Modelo exportado a GGUF
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_model.py
â”‚   â”œâ”€â”€ fine_tuning.py
â”‚   â”œâ”€â”€ merge_models.py
â”‚   â”œâ”€â”€ convert_hf_to_gguf.py
â”‚   â”œâ”€â”€ gguf_writer.py
â”‚   â””â”€â”€ vllm_test.py
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## ğŸš€ Flujo de trabajo

### 1. Descargar el modelo base

```bash
python scripts/download_model.py
```

Descarga **`mistralai/Mistralâ€‘7Bâ€‘Instructâ€‘v0.3`** en `./models/mistral/`.

---

### 2. Fineâ€‘tuning con datos de telecomunicaciones

```bash
python scripts/fine_tuning.py
```

Entrena el modelo con los archivos del directorio `dataset/`. El resultado se guarda en `./models/mistral-7b-finetuned/`.

---

### 3. FusiÃ³n del modelo base y LoRA

```bash
python scripts/merge_models.py
```

Genera `./models/mistral-7b-merged/` con los pesos consolidados.

---

### 4. Pruebas de carga concurrente

```bash
python scripts/vllm_test.py --threads 10
```

El script lanza 10 hilos simultÃ¡neos usando la API de `vLLM` y muestra *throughput* y *latency* antes y despuÃ©s del fineâ€‘tuning.

---

### 5. Despliegue con DockerÂ +Â vLLM

AsegÃºrate de que la ruta configurada en `docker-compose.yml` apunte a `./models/mistral-7b-merged/`, luego:

```bash
docker compose up -d
```

El endpoint REST estarÃ¡ en <http://localhost:8000/v1>.

---

### 6. ConversiÃ³n a GGUF

```bash
python scripts/convert_hf_to_gguf.py   --dir-model ./models/mistral-7b-merged   --outfile   ./models/mistral-7b-gguf/mistral-7b-telecom.gguf   --outtype   f16
```

El archivo `.gguf` resultante es compatible con **LMÂ Studio** y `llama.cpp`.

---

## ğŸ§ª Ejemplo de evaluaciÃ³n

```python
MESSAGES = [
    {
        "role": "system",
        "content": "Eres un ingeniero experto en telecomunicaciones..."
    },
    {
        "role": "user",
        "content": "Describe un problema tÃ­pico en redes GPON o HFC."
    }
]
```

- **Sin fineâ€‘tuning:** respuestas vagas o alucinaciones.  
- **Con fineâ€‘tuning:** explicaciones tÃ©cnicas, referencias correctas a GPON/HFC y mejores prÃ¡cticas.

---

## ğŸ§° DescripciÃ³n de scripts

| Script | DescripciÃ³n |
|--------|-------------|
| `download_model.py` | Descarga el modelo base desde Hugging Face. |
| `fine_tuning.py` | Aplica LoRA + cuantizaciÃ³n 4â€‘bit. |
| `merge_models.py` | Fusiona pesos LoRA y produce el modelo final. |
| `vllm_test.py` | Benchmark multihilo usando la API de `vLLM`. |
| `convert_hf_to_gguf.py` | Convierte el modelo a GGUF. |
| `gguf_writer.py` | Utilidad auxiliar para la conversiÃ³n. |

---

## ğŸ“Œ Notas

- El tokenizer de Mistral no trae `pad_token`; los scripts lo aÃ±aden automÃ¡ticamente.
- DespuÃ©s de convertir a GGUF, carga el modelo en **LMÂ Studio** sin pasos adicionales.
- Ajusta los hiperparÃ¡metros de `fine_tuning.py` (batch size, lr, epochs) segÃºn tu GPU.

---

## ğŸ¤ CrÃ©ditos

Proyecto basado en:

- [Hugging Face Transformers]  
- [vLLM Project]  
- [PEFT / LoRA]  
- [llama.cpp]

---

## ğŸ“œ Licencia

Este repositorio se proporciona Ãºnicamente con fines educativos y de experimentaciÃ³n. Verifica y respeta las licencias y tÃ©rminos de uso del modelo base y de cada dependencia.
