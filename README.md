# ğŸ“¡ vLLM + Fine-Tuning de Mistral-7B para Telecomunicaciones

Este repositorio contiene el flujo completo para descargar, afinar, fusionar y desplegar **Mistral-7B-Instruct-v0.3** con conocimientos especÃ­ficos del dominio de telecomunicaciones.

## âœ¨ Funcionalidades

- **Descarga** automÃ¡tica del modelo base desde Hugging Face.
- **Fine-tuning** con LoRA (4-bit) empleando los conjuntos `qa_dataset.csv` y `glosario_dataset.csv`.
- **FusiÃ³n (merge)** de los adaptadores LoRA con el modelo original.
- **Despliegue** en `vLLM` para inferencia concurrente.
- **ConversiÃ³n** a formato **GGUF** compatible con **LM Studio** o `llama.cpp`.

## ğŸ–¥ï¸ Requisitos

| Recurso | RecomendaciÃ³n mÃ­nima |
|---------|----------------------|
| GPU     | NVIDIA RTX 3080 (â‰¥16Â GB VRAM) |
| CPU     | 8 nÃºcleos |
| RAM     | 32Â GB |
| SO      | Linux, WSLÂ 2 o Windows 10+ |
| Software| PythonÂ 3.10+, Docker |

Instala las dependencias de Python:

```bash
pip install -r requirements.txt
```

## ğŸ“ Estructura del proyecto

```text
.
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ qa_dataset.csv
â”‚   â””â”€â”€ glosario_dataset.csv
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ mistral/              # Modelo descargado de HF
â”‚   â”œâ”€â”€ mistral-7b-finetuned/ # Modelo despuÃ©s del fineâ€‘tuning
â”‚   â”œâ”€â”€ mistral-7b-merged/    # Modelo con LoRA fusionado
â”‚   â””â”€â”€ mistral-7b-gguf/      # Modelo exportado a GGUF
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_model.py
â”‚   â”œâ”€â”€ fine_tuning.py
â”‚   â”œâ”€â”€ merge_models.py
â”‚   â”œâ”€â”€ convert_hf_to_gguf.py
â”‚   â”œâ”€â”€ gguf_writer.py
â”‚   â””â”€â”€ vllm_test.py
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

## ğŸš€ Flujo de trabajo

### 1. Descargar el modelo base

```bash
python scripts/download_model.py
```

Descarga **`mistralai/Mistral-7B-Instruct-v0.3`** en `./models/mistral/`.

### 2. Fineâ€‘tuning con datos de telecomunicaciones

```bash
python scripts/fine_tuning.py
```

Entrena el modelo con los archivos del directorio `dataset/`. El resultado se guarda en `./models/mistral-7b-finetuned/`.

### 3. FusiÃ³n del modelo base y LoRA

```bash
python scripts/merge_models.py
```

Genera `./models/mistral-7b-merged/` con los pesos consolidados.

### 4. Pruebas de carga concurrente

```bash
python scripts/vllm_test.py --threads 10
```

El script lanza 10 hilos simultÃ¡neos usando la API de `vLLM` y muestra *throughput* y *latency* antes y despuÃ©s del fine-tuning.

### 5. Despliegue con Docker + vLLM

AsegÃºrate de que la ruta configurada en `docker-compose.yml` apunte a `./models/mistral-7b-merged/`, luego:

```bash
docker compose up -d
```

El endpoint REST estarÃ¡ en <http://localhost:8000/v1>.

### 6. ConversiÃ³n a GGUF

```bash
python scripts/convert_hf_to_gguf.py   --dir-model ./models/mistral-7b-merged   --outfile   ./models/mistral-7b-gguf/mistral-7b-telecom.gguf   --outtype   f16
```

El archivo `.gguf` resultante es compatible con **LM Studio** y `llama.cpp`.

## ğŸ§ª Ejemplo de evaluaciÃ³n

```python
MESSAGES = [
    {
        "role": "system",
        "content": "Eres un ingeniero experto en telecomunicaciones..."
    },
    {
        "role": "user",
        "content": "Describe un problema tÃ­pico en redes GPON o HFC."
    }
]
```

- **Sin fineâ€‘tuning:** respuestas vagas o alucinaciones.  
- **Con fineâ€‘tuning:** explicaciones tÃ©cnicas, referencias correctas a GPON/HFC y mejores prÃ¡cticas.

## ğŸ§° DescripciÃ³n de scripts

| Script | DescripciÃ³n |
|--------|-------------|
| `download_model.py` | Descarga el modelo base desde Hugging Face. |
| `fine_tuning.py` | Aplica LoRA + cuantizaciÃ³n 4â€‘bit. |
| `merge_models.py` | Fusiona pesos LoRA y produce el modelo final. |
| `vllm_test.py` | Benchmark multihilo usando la API de `vLLM`. |
| `convert_hf_to_gguf.py` | Convierte el modelo a GGUF. |
| `gguf_writer.py` | Utilidad auxiliar para la conversiÃ³n. |

## ğŸ“Œ Notas

- El tokenizer de Mistral no trae `pad_token`; los scripts lo aÃ±aden automÃ¡ticamente.
- DespuÃ©s de convertir a GGUF, carga el modelo en **LM Studio** sin pasos adicionales.
- Ajusta los hiperparÃ¡metros de `fine_tuning.py` (batch size, lr, epochs) segÃºn tu GPU.

## ğŸ¤ CrÃ©ditos

Proyecto basado en:

- [Hugging Face Transformers]  
- [vLLM Project]  
- [PEFT / LoRA]  
- [llama.cpp]

## ğŸ“œ Licencia

Este repositorio se proporciona Ãºnicamente con fines educativos y de experimentaciÃ³n. Verifica y respeta las licencias y tÃ©rminos de uso del modelo base y de cada dependencia.


---

# ğŸ“¡ vLLM + Mistralâ€‘7B Fineâ€‘Tuning for Telecommunications

This repository provides an endâ€‘toâ€‘end workflow to download, fineâ€‘tune, merge, and deploy **Mistralâ€‘7Bâ€‘Instructâ€‘v0.3** with domainâ€‘specific knowledge in telecommunications.

## âœ¨ Features

- **Automatic download** of the base model from Hugging Face.
- **Fineâ€‘tuning** with LoRA (4â€‘bit) using `qa_dataset.csv` and `glosario_dataset.csv`.
- **Merge** of LoRA adapters with the original model.
- **Deployment** on `vLLM` for concurrent inference.
- **Conversion** to **GGUF** format for **LM Studio** or `llama.cpp`.

## ğŸ–¥ï¸ Requirements

| Resource | Minimum recommendation |
|----------|------------------------|
| GPU      | NVIDIA RTXÂ 3080 (â‰¥16Â GB VRAM) |
| CPU      | 8 cores |
| RAM      | 32Â GB |
| OS       | Linux, WSLÂ 2, or WindowsÂ 10+ |
| Software | PythonÂ 3.10+, Docker |

Install Python dependencies:

```bash
pip install -r requirements.txt
```

## ğŸ“ Project structure

```text
.
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ qa_dataset.csv
â”‚   â””â”€â”€ glosario_dataset.csv
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ mistral/              # Model downloaded from HF
â”‚   â”œâ”€â”€ mistral-7b-finetuned/ # Model after fineâ€‘tuning
â”‚   â”œâ”€â”€ mistral-7b-merged/    # Model with LoRA merged
â”‚   â””â”€â”€ mistral-7b-gguf/      # Model exported to GGUF
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_model.py
â”‚   â”œâ”€â”€ fine_tuning.py
â”‚   â”œâ”€â”€ merge_models.py
â”‚   â”œâ”€â”€ convert_hf_to_gguf.py
â”‚   â”œâ”€â”€ gguf_writer.py
â”‚   â””â”€â”€ vllm_test.py
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

## ğŸš€ Workflow

### 1. Download the base model

```bash
python scripts/download_model.py
```

Downloads **`mistralai/Mistral-7B-Instruct-v0.3`** into `./models/mistral/`.

### 2. Fineâ€‘tune with telecom data

```bash
python scripts/fine_tuning.py
```

Trains the model using files in `dataset/`. The result is saved to `./models/mistral-7b-finetuned/`.

### 3. Merge base model and LoRA

```bash
python scripts/merge_models.py
```

Creates `./models/mistral-7b-merged/` with consolidated weights.

### 4. Concurrent load testing

```bash
python scripts/vllm_test.py --threads 10
```

The script launches 10 concurrent threads via the `vLLM` API, reporting *throughput* and *latency* before and after fineâ€‘tuning.

### 5. Deploy with Docker + vLLM

Ensure the path in `docker-compose.yml` points to `./models/mistral-7b-merged/`, then run:

```bash
docker compose up -d
```

The REST endpoint will be available at <http://localhost:8000/v1>.

### 6. Convert to GGUF

```bash
python scripts/convert_hf_to_gguf.py   --dir-model ./models/mistral-7b-merged   --outfile   ./models/mistral-7b-gguf/mistral-7b-telecom.gguf   --outtype   f16
```

The resulting `.gguf` file is compatible with **LM Studio** and `llama.cpp`.

## ğŸ§ª Evaluation example

```python
MESSAGES = [
    {
        "role": "system",
        "content": "You are a telecommunications engineering expert..."
    },
    {
        "role": "user",
        "content": "Describe a typical issue in GPON or HFC networks."
    }
]
```

- **Without fineâ€‘tuning:** vague answers or hallucinations.  
- **With fineâ€‘tuning:** technical explanations, correct references to GPON/HFC, and best practices.

## ğŸ§° Script overview

| Script | Description |
|--------|-------------|
| `download_model.py` | Downloads the base model from Hugging Face. |
| `fine_tuning.py` | Applies LoRA + 4â€‘bit quantization. |
| `merge_models.py` | Merges LoRA weights to produce the final model. |
| `vllm_test.py` | Multiâ€‘thread benchmark using the `vLLM` API. |
| `convert_hf_to_gguf.py` | Converts the model to GGUF. |
| `gguf_writer.py` | Helper utility for conversion. |

## ğŸ“Œ Notes

- Mistralâ€™s tokenizer lacks a `pad_token`; the scripts add one automatically.
- After conversion to GGUF, load the model in **LM Studio** without extra steps.
- Adjust hyperparameters in `fine_tuning.py` (batch size, lr, epochs) according to your GPU.

## ğŸ¤ Credits

Project built with:

- [Hugging Face Transformers]  
- [vLLM Project]  
- [PEFT / LoRA]  
- [llama.cpp]

## ğŸ“œ License

This repository is provided for educational and experimental purposes only. Check and respect the licenses and terms of use of the base model and each dependency.

