# ğŸ“¡ vLLM + Fine-tuning de Mistral 7B para Telecomunicaciones

Este repositorio permite:

- Descargar y preparar el modelo **Mistral-7B-Instruct-v0.3** desde Hugging Face.
- Realizar **fine-tuning especializado en telecomunicaciones** utilizando LoRA con cuantizaciÃ³n en 4 bits.
- Unir el modelo original con los pesos ajustados (merge).
- Desplegar el modelo en una instancia de `vLLM` para inferencia concurrente.
- Convertir el modelo final al formato **GGUF** para uso en **LM Studio**.

---

## ğŸ–¥ï¸ Requisitos

- GPU NVIDIA RTX 3080 o superior (mÃ­nimo 16 GB VRAM)
- Python 3.10+
- Docker
- pip packages:
  ```bash
  pip install -r requirements.txt
````

---

## ğŸ“ Estructura del Proyecto

```
.
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ qa_dataset.csv
â”‚   â””â”€â”€ glosario_dataset.csv
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ mistral/                 # Modelo descargado desde HF
â”‚   â”œâ”€â”€ mistral-7b-finetuned/    # Modelo despuÃ©s del fine-tuning
â”‚   â”œâ”€â”€ mistral-7b-merged/       # Modelo con LoRA fusionado
â”‚   â””â”€â”€ mistral-7b-gguf/         # Modelo exportado a GGUF
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_model.py
â”‚   â”œâ”€â”€ fine_tuning.py
â”‚   â”œâ”€â”€ merge_models.py
â”‚   â”œâ”€â”€ convert_hf_to_gguf.py
â”‚   â”œâ”€â”€ gguf_writer.py
â”‚   â””â”€â”€ vLLM_Test.py
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## ğŸš€ Flujo de Trabajo

### 1. ğŸ“¥ Descargar el Modelo Base

```bash
python download_model.py
```

Esto descargarÃ¡ `mistralai/Mistral-7B-Instruct-v0.3` a `./models/mistral/`.

---

### 2. ğŸ§  Fine-tuning con Datos de Telecomunicaciones

```bash
python fine_tuning.py
```

Entrena el modelo con `qa_dataset.csv` y `glosario_dataset.csv`. El modelo ajustado se guarda en `./models/mistral-7b-finetuned/`.

---

### 3. ğŸ”— FusiÃ³n del Modelo Base + LoRA

```bash
python merge_models.py
```

Esto crea un modelo final en `./models/mistral-7b-merged/` con los pesos combinados.

---

### 4. ğŸ§ª Pruebas de Sesiones SimultÃ¡neas

Lanza 10 hilos concurrentes con peticiones tipo Chat OpenAI para medir desempeÃ±o del modelo usando `vLLM`.

```bash
python vLLM_Test.py
```

ğŸ“Œ **Resultados esperados**:

* Antes del fine-tuning: El modelo â€œalucinaâ€ al hablar de telecomunicaciones.
* DespuÃ©s del fine-tuning: Da definiciones precisas y relevantes de redes como **HFC**, **GPON**, etc.

---

### 5. ğŸ³ Despliegue con Docker y vLLM

AsegÃºrate de que el path en `docker-compose.yml` apunte a `./models/mistral-7b-merged/`.

```bash
docker-compose up -d
```

El API estarÃ¡ disponible en: [http://localhost:8000/v1](http://localhost:8000/v1)

---

### 6. ğŸ”„ ConversiÃ³n a GGUF para LM Studio

```bash
python convert_hf_to_gguf.py \
  --dir-model ./models/mistral-7b-merged \
  --outfile ./models/mistral-7b-gguf/mistral-7b-telecom.gguf \
  --outtype f16
```

Esto genera un archivo `.gguf` compatible con LM Studio o llama.cpp.

---

## ğŸ§ª EvaluaciÃ³n del Fine-tuning

El archivo `vLLM_Test.py` demuestra cÃ³mo evaluar la capacidad del modelo antes y despuÃ©s del ajuste.

```python
MESSAGES = [
    {"role": "system", "content": "Eres un ingeniero experto en telecomunicaciones..."},
    {"role": "user", "content": "Describe un problema tÃ­pico en redes GPON o HFC."}
]
```

DespuÃ©s del entrenamiento, el modelo responde con precisiÃ³n tÃ©cnica en espaÃ±ol, sin inventar informaciÃ³n.

---

## ğŸ§° Scripts Incluidos

* `download_model.py`: descarga el modelo base desde Hugging Face.
* `fine_tuning.py`: realiza fine-tuning con LoRA + 4-bit quantization.
* `merge_models.py`: fusiona el modelo base con el adaptador LoRA.
* `vLLM_Test.py`: lanza pruebas multihilo vÃ­a API de vLLM.
* `convert_hf_to_gguf.py` y `gguf_writer.py`: convierten el modelo al formato GGUF.

---

## ğŸ“Œ Notas

* El tokenizer de Mistral no incluye `pad_token` por defecto; se ajusta automÃ¡ticamente en los scripts.
* El modelo final puede ser usado en **LM Studio** sin problemas luego de la conversiÃ³n.

---

## ğŸ¤ CrÃ©ditos

Este proyecto combina herramientas de:

* [HuggingFace Transformers](https://huggingface.co)
* [vLLM](https://github.com/vllm-project/vllm)
* [PEFT / LoRA](https://github.com/huggingface/peft)
* [llama.cpp](https://github.com/ggerganov/llama.cpp)

---

## ğŸ“œ Licencia

Este repositorio es solo para fines educativos y de experimentaciÃ³n. Respeta las licencias de cada modelo base utilizado.